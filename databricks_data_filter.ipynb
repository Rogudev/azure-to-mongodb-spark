{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cc73b7-dbfb-4dc4-b274-97b0e3faad45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /databricks/python3/lib/python3.9/site-packages (from pandas) (1.21.5)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d276bec2-d6a0-4205-b9c3-937db243b6d1/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Requirement already satisfied: unidecode in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d276bec2-d6a0-4205-b9c3-937db243b6d1/lib/python3.9/site-packages (1.3.8)\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-d276bec2-d6a0-4205-b9c3-937db243b6d1/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3713bec-4c27-4022-aba0-9ffcf24a9646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4481b781-7fa8-4c0a-bfb1-beca3441fd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize spark session\n",
    "spark = SparkSession.builder.appName(\"Mongo DB Uploader\").getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f30e38f-bff9-439e-b6a9-512b99682659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV scheme\n",
    "csv_Scheme = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"start_time\", StringType(), True),  \n",
    "    StructField(\"end_time\", StringType(), True),    \n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"exercise_title\", StringType(), True),\n",
    "    StructField(\"superset_id\", StringType(), True),\n",
    "    StructField(\"exercise_notes\", StringType(), True),\n",
    "    StructField(\"set_index\", StringType(), True),\n",
    "    StructField(\"set_type\", StringType(), True),\n",
    "    StructField(\"weight_kg\", FloatType(), True),\n",
    "    StructField(\"reps\", FloatType(), True),\n",
    "    StructField(\"distance_km\", FloatType(), True),\n",
    "    StructField(\"duration_seconds\", FloatType(), True),\n",
    "    StructField(\"rpe\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef951a00-cffe-4dae-9074-fbc782a68e29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cloud data\n",
    "storage_account_name = \"<YOUR_ACCOUNT_NAME>\"  \n",
    "storage_account_key = \"<YOUR_ACCOUNT_KEY>\" \n",
    "container_name = \"<YOUR_CONTAINER_NAME>\"  \n",
    "\n",
    "# configure credentials\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf25fc8-1664-4e53-9900-76b37d4aa5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# path of inputs\n",
    "csvPath = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b0a2d5-7748-4981-9239-1e9388dff9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pandas functions\n",
    "def drop_cols(df, cols):\n",
    "    cols_to_drop = cols\n",
    "\n",
    "    for col in cols_to_drop:\n",
    "        df = df.drop(col, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fragment_df(df, exercises):\n",
    "\n",
    "    exercises_dfs = []\n",
    "\n",
    "    for exercise in exercises:\n",
    "        # select rows using the name of the exercise\n",
    "        exercises_dfs.append( df[df['exercise_title'] == exercise] )\n",
    "\n",
    "    return exercises_dfs\n",
    "\n",
    "def process_na_column(df, cols):\n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # iterate all cols\n",
    "    for col in cols:\n",
    "        # iterate df rows\n",
    "        for index, row in df.iterrows():\n",
    "            # check na\n",
    "            if pd.isna(row[col]):\n",
    "                # first row\n",
    "                if index == 0:\n",
    "                    next_non_na = None\n",
    "                    # get the next value doing an iteration between index+1 (next_index) and the df length\n",
    "                    for next_index in range(index + 1, len(df)):\n",
    "                        # if its not a na, fill value and break loop\n",
    "                        if not pd.isna(df.at[next_index, col]):\n",
    "                            next_non_na = df.at[next_index, col]\n",
    "                            # set the value\n",
    "                            df.at[index, col] = next_non_na\n",
    "                            break\n",
    "\n",
    "                elif index > 0:\n",
    "                    # get the prev value\n",
    "                    prev_non_na = df.at[index-1, col]\n",
    "\n",
    "                    next_non_na = None\n",
    "                    # if col is not a text and the index is not the same as last row, use the prev and the following data to create the mean\n",
    "                    if (col == 'weight_kg' or col == 'reps') and index != len(df):\n",
    "                        # to get the next value, use the same as for index == 0\n",
    "                       \n",
    "                        for next_index in range(index + 1, len(df)):\n",
    "                            # if its not a na, fill value and break loop\n",
    "                            if not pd.isna(df.at[next_index, col]):\n",
    "                                next_non_na = df.at[next_index, col]\n",
    "                                # set the value\n",
    "                                df.at[index, col] = next_non_na\n",
    "                                break\n",
    "                    # if we have a next no nan, create the mean       \n",
    "                    if next_non_na:\n",
    "                        mean = (prev_non_na + next_non_na) /2\n",
    "                        df.at[index, col] = mean\n",
    "                    else:\n",
    "                        # set the value\n",
    "                        df.at[index, col] = prev_non_na\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_months_es_to_en(date_str):\n",
    "    months_es = {\n",
    "        'ene': 'Jan', 'feb': 'Feb', 'mar': 'Mar', 'abr': 'Apr', 'may': 'May', 'jun': 'Jun',\n",
    "        'jul': 'Jul', 'ago': 'Aug', 'sep': 'Sep', 'oct': 'Oct', 'nov': 'Nov', 'dic': 'Dec'\n",
    "    }\n",
    "    \n",
    "    for es_month, en_month in months_es.items():\n",
    "        date_str = date_str.replace(es_month, en_month)\n",
    "    \n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14af9389-e1a1-4003-8e42-72c3ce9c1abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# write in MongoAtlas\n",
    "def write_row(df, batch_id):\n",
    "    # convert spark df to pandas df\n",
    "    pandas_df = df.toPandas()\n",
    "\n",
    "    # drop unused cols\n",
    "    cols_to_drop = ['title', 'end_time', 'description', 'set_type', 'superset_id', 'exercise_notes', 'set_index', 'distance_km', 'duration_seconds', 'rpe']\n",
    "    pandas_df = drop_cols(pandas_df, cols_to_drop)\n",
    "\n",
    "    # drop row if exercise_title is None\n",
    "    pandas_df = pandas_df.dropna(subset=['exercise_title'])\n",
    "\n",
    "    '''\n",
    "    create a df for every exercise, that's for upload every df in their correspondant collection\n",
    "    '''\n",
    "    # create a list using the exercises col\n",
    "    exercises_list = pandas_df['exercise_title'].dropna().unique().tolist()\n",
    "    # fragment the big df into little df, 1 for exercise and save it into 'exercises_df_list'\n",
    "    exercises_df_list = fragment_df(pandas_df, exercises_list)\n",
    "\n",
    "    # process cols\n",
    "    scheme_cols = ['start_time', 'exercise_title', 'weight_kg', 'reps']\n",
    "\n",
    "    # For each exercise dataframe, process the NaN columns\n",
    "    for i in range(len(exercises_df_list)):\n",
    "        # Replace the old df with the processed df (no need to overwrite the entire list)\n",
    "        exercises_df_list[i] = process_na_column(exercises_df_list[i], scheme_cols)\n",
    "\n",
    "        # replace the Spanish months to eng months\n",
    "        exercises_df_list[i]['start_time'] = exercises_df_list[i]['start_time'].apply(replace_months_es_to_en)\n",
    "        # convert to datetime\n",
    "        exercises_df_list[i]['start_time'] = pd.to_datetime(exercises_df_list[i]['start_time'], format='%d %b %Y, %H:%M')\n",
    "\n",
    "    index = 0\n",
    "    for pandas_df in exercises_df_list:\n",
    "\n",
    "        # get the exercise and format to get the correct collection\n",
    "        collection = unidecode(pandas_df.head(1)['exercise_title'][0].lower().replace(' ', '_').replace('(', '').replace(')', ''))\n",
    "        print(f'\\t{index}| Uploading {collection} collection...')\n",
    "\n",
    "        # convert to spark df\n",
    "        spark_df = spark.createDataFrame(pandas_df)\n",
    "        \n",
    "        # upload to mongo\n",
    "        spark_df.write.format(\"mongodb\").mode(\"append\") \\\n",
    "            .option(\"connection.uri\", \"<YOUR_MONGO_URI>\")\\\n",
    "            .option(\"database\", \"<YOUR_DATABASE>\") \\\n",
    "            .option(\"collection\", f\"{collection}\") \\\n",
    "            .save()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d8971e-6dae-4734-90e9-5cf3620c2260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read CSV in streamming\n",
    "df = spark.readStream \\\n",
    "    .schema(csv_Scheme) \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('maxFilesPerTrigger', 1) \\\n",
    "    .csv(csvPath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a3b4de-7b2d-4d95-8a14-aa8edc9814ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0| Uploading jalon_al_pecho_maquina collection...\n",
      "\t0| Uploading remo_inclinado_barra collection...\n",
      "\t0| Uploading remo_con_mancuerna collection...\n",
      "\t0| Uploading remo_sentado_con_cable collection...\n",
      "\t0| Uploading straight_arm_lat_pulldown_cable collection...\n"
     ]
    }
   ],
   "source": [
    "# stream configuration\n",
    "checkpoint_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/checkpoints/\"\n",
    "# the checkpoints avoids re readings\n",
    "df.writeStream \\\n",
    "    .foreachBatch(write_row) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) EvaluableEjPropio",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
